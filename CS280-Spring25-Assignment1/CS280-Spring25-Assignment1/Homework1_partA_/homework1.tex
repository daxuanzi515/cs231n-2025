\documentclass[12pt]{article}%
\usepackage{amsfonts}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=2.2cm, right=2.2cm]%
{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{changepage}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}%
\usepackage{xcolor}
\usepackage{amsmath} 
\setcounter{MaxMatrixCols}{30}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\title{CS280 Spring 2025 Assignment 1 \\ Part A}
\author{Basics}
\maketitle

\paragraph{Name: Chen Xuanxin}

\paragraph{Student ID: 2024233125}

\newpage

\subsubsection*{1. \textit{Maximum Likelihood Estimation} (10 points).}
Consider a dataset $\mathcal{D}$ consisting of $n$ independent and identically distributed samples:
\begin{equation}
    \mathcal{D} = \left\{ ((x^1_1, x^1_2), y^1), ((x^2_1, x^2_2), y^2), \ldots, ((x^n_1, x^n_2), y^n) \right\},
    \tag{1}
\end{equation}
where $(x^i_1, x^i_2) \in \mathbb{R}^2$ are input features and $y^i \in \mathbb{R}$ is an output. 
\\
Assume that every output $y^i$ in $\mathcal{D}$ is generated by inputting $(x^i_1, x^i_2)$ into a model:
\begin{equation}
    y = f_{\theta_1,\theta_2}(x_1, x_2) + \epsilon,
    \tag{2}
\end{equation}
where the function $f_{\theta_1,\theta_2}$ is a mapping from features $(x_1, x_2) \in \mathbb{R}^2$ to a value in $\mathbb{R}$, which has two parameters $\theta_1$ and $\theta_2$. Here we assume that the random noise $\epsilon \sim N(0,\sigma^2)$ is independent and distributed according to a Gaussian distribution with zero mean and variance $\sigma^2$.
\vspace{1em}
\\
(a) Show that the log likelihood of the data given the parameters is:
\begin{equation}
    l(\mathcal{D};\theta_1,\theta_2) = -\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y^i-f_{\theta_1,\theta_2}(x^i_1,x^i_2))^2-n\log(\sqrt{2\pi}\sigma).
    \tag{3}
\end{equation}
Recall the probability density function of the Gaussian distribution $N(\mu,\sigma^2)$ is:
\begin{equation}
    p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right).
    \tag{4}
\end{equation}
\vspace{1em}
\\
(b) To find the maximum likelihood estimates of $\theta_1$ and $\theta_2$ using gradient descent, compute the gradient of the log likelihood with respect to $\theta_1$ and $\theta_2$. Express you answer in terms of:
\begin{equation*}
    y^i, \enspace f_{\theta_1,\theta_2}(x^i_1, x^i_2), \enspace \frac{\partial}{\partial\theta_1}f_{\theta_1,\theta_2}(x^i_1, x^i_2), \enspace \frac{\partial}{\partial\theta_2}f_{\theta_1,\theta_2}(x^i_1, x^i_2)
\end{equation*}
\vspace{1em}
\\
(c) Given the learning rate $\eta$, what update rule would you use in gradient descent to \textit{maximize} the likelihood.

\pagebreak

\input{answers/answer_A.tex}

\pagebreak

\subsubsection*{2. \textit{Loss Function} (10 points).}

Assume that a classifier is written as $H(x) = sign(F(x))$, where $H(x):\mathbb{R}^d \rightarrow \{-1, 1\}$, $sign()$ is a sign function, and $F(x): \mathbb{R}^d \rightarrow \mathbb{R}$. To obtain the parameters in $F(x)$, we need to minimize the loss function averaged over the training set: ${\textstyle \sum_{i}^{}}L(y^iF(x^i))$. Here $L$ is a function of $yF(x)$. For example, for linear classifiers, $F(x)=w_0 + {\textstyle \sum_{j=1}^{d}}w_jx_j$, and $yF(x) = y(w_0 + {\textstyle \sum_{j=1}^{d}}w_jx_j)$.
\vspace{1em}
\\
(a) Which loss functions below are appropriate to use in classification? For the ones that are not appropriate, explain why not. In general, what conditions does $L$ have to satisfy in order to be an appropriate loss function? The x axis is $yF(x)$, and the y axis is $L(yF(x))$.
\begin{figure}[htpb]
    \centering
    \includegraphics[scale=0.45]{partA_loss.png}
    \label{fig:enter-label}
\end{figure}
\vspace{1em}
\\
(b) Among the above loss functions appropriate to use in classification, which one is the most robust to outliers? Justify your answer.
\vspace{1em}
\\
(c) Let $F(x)=w_0+\sum_{j=1}^{d}w_jx_j$ and $L(yF(x))= \frac{1}{1+exp(yF(x))} $. Suppose you use gradient descent to obtain the optimal values for $w_0$ and $w_j$. Give the update rules for these parameters.
\pagebreak
\input{answers/answer_B.tex}
\end{document}